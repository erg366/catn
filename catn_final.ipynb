{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import markovify\n",
    "import tracery\n",
    "from tracery.modifiers import base_english\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "words = []\n",
    "noun_chunks = []\n",
    "entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3734\n",
      "100 3734\n",
      "200 3734\n",
      "300 3734\n",
      "400 3734\n",
      "500 3734\n",
      "600 3734\n",
      "700 3734\n",
      "800 3734\n",
      "900 3734\n"
     ]
    }
   ],
   "source": [
    "rep_sentences = []\n",
    "\n",
    "the_republic_text = open(\"the_republic.txt\").read()\n",
    "republic_sent = the_republic_text.split(\".\")\n",
    "\n",
    "rep_words = []\n",
    "rep_noun_chunks = []\n",
    "rep_entities = []\n",
    "rep_name_entities = []\n",
    "for i, sent in enumerate(random.sample(republic_sent, 1000)):\n",
    "    rep_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(republic_sent))\n",
    "    doc = nlp(sent)\n",
    "    rep_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    rep_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    rep_name_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ardiaeus Ardiaeus Cretic Thrasymachus Hesiod Socrates Lysanias Socrates Thrasymachus Socrates Thrasymachus Thrasymachus Shall Odyssey ye wretches dissolution:--In Agamemnon Agamemnon Xerxes Socrates Thrasymachus Polemarchus Glaucon Nay the Creator Thrasymachus Asclepius Odysseus Thrasymachus Thrasymachus Glaucon Homer Protagoras Glaucon Pythagoras Lysias Euthydemus Thrasymachus Cleitophon the son Aristonymus Asclepius Socrates Muses Zeus Thrasymachus Telamon Love Themistocles Thrasymachus Socrates Glaucon Socrates Socrates Glaucon Agamemnon sweet Adeimantus Socrates Glaucon Thrasymachus Socrates Socrates Socrates Ariston Behold Socrates Glaucon Thrasymachus Socrates Socrates Piraeus Agamemnon Hellas Glaucon Ardiaeus Ardiaeus Atreus Polemarchus \n"
     ]
    }
   ],
   "source": [
    "people = [e for e in rep_name_entities if e.label_ == \"PERSON\"]\n",
    "namestring = \"\"\n",
    "for entity in rep_name_entities:\n",
    "    if entity in people:\n",
    "        namestring += entity.text + \" \"\n",
    "    else:\n",
    "        rep_entities.append(entity)\n",
    "print(namestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2082\n",
      "100 2082\n",
      "200 2082\n",
      "300 2082\n",
      "400 2082\n",
      "500 2082\n",
      "600 2082\n",
      "700 2082\n",
      "800 2082\n",
      "900 2082\n"
     ]
    }
   ],
   "source": [
    "ym_sentences = []\n",
    "\n",
    "your_mind_text = open(\"your_mind.txt\").read()\n",
    "yourmind_sent = your_mind_text.split(\".\")\n",
    "\n",
    "ym_words = []\n",
    "ym_noun_chunks = []\n",
    "ym_entities = []\n",
    "ym_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(random.sample(yourmind_sent, 1000)):\n",
    "    ym_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(yourmind_sent))\n",
    "    doc = nlp(sent)\n",
    "    ym_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    ym_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    ym_name_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jevons Green Ordinarily medulla --An Cinderella Cinderella Aaron Burr Halleck Desire babe John Smith John Smith James Davidson John\n",
      "Smith John Smith Snow Benedict Andr√© Jeppe Grasp William James William Hamilton Learn bush Halleck Major Minor premise_)--The William James Smiths Smith John Smith John\n",
      "Simpson Hoffding Huxley Grant Allen Feelings Halleck Smiths Smith --_Abraham Lincoln Gordy Spencer Contrariwise reverie Brooks John Smith Henry Jones vogel_ John Law Brooks Halleck Jevons John Fiske's civic virtue Halleck --In Mitchell James \n"
     ]
    }
   ],
   "source": [
    "people = [e for e in ym_name_entities if e.label_ == \"PERSON\"]\n",
    "namestring = \"\"\n",
    "for entity in ym_name_entities:\n",
    "    if entity in people:\n",
    "        namestring += entity.text + \" \"\n",
    "    else:\n",
    "        ym_entities.append(entity)\n",
    "print(namestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 663\n",
      "100 663\n",
      "200 663\n",
      "300 663\n",
      "400 663\n",
      "500 663\n",
      "600 663\n"
     ]
    }
   ],
   "source": [
    "mam_sentences = []\n",
    "\n",
    "mam_text = open(\"man_a_machine.txt\").read()\n",
    "mam_sent = mam_text.split(\".\")\n",
    "\n",
    "mam_words = []\n",
    "mam_noun_chunks = []\n",
    "mam_entities = []\n",
    "mam_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(mam_sent):\n",
    "    mam_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(mam_sent))\n",
    "    doc = nlp(sent)\n",
    "    mam_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    mam_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    mam_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 341\n",
      "100 341\n",
      "200 341\n",
      "300 341\n"
     ]
    }
   ],
   "source": [
    "tlp_sentences = []\n",
    "\n",
    "tlp_text = open(\"the_love_poems.txt\").read()\n",
    "tlp_sent = tlp_text.split(\".\")\n",
    "\n",
    "tlp_words = []\n",
    "tlp_noun_chunks = []\n",
    "tlp_entities = []\n",
    "tlp_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(tlp_sent):\n",
    "    tlp_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(tlp_sent))\n",
    "    doc = nlp(sent)\n",
    "    tlp_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    tlp_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    tlp_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 389\n",
      "100 389\n",
      "200 389\n",
      "300 389\n"
     ]
    }
   ],
   "source": [
    "rts_sentences = []\n",
    "\n",
    "rts_text = open(\"rivers_to_the_sea.txt\").read()\n",
    "rts_sent = rts_text.split(\".\")\n",
    "\n",
    "rts_words = []\n",
    "rts_noun_chunks = []\n",
    "rts_entities = []\n",
    "rts_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(rts_sent):\n",
    "    rts_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(rts_sent))\n",
    "    doc = nlp(sent)\n",
    "    rts_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    rts_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    rts_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 62\n"
     ]
    }
   ],
   "source": [
    "ibm_sentences = []\n",
    "\n",
    "ibm_text = open(\"ibm-1401.txt\").read()\n",
    "ibm_sent = ibm_text.split(\".\")\n",
    "\n",
    "ibm_words = []\n",
    "ibm_noun_chunks = []\n",
    "ibm_entities = []\n",
    "ibm_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(ibm_sent):\n",
    "    ibm_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(ibm_sent))\n",
    "    doc = nlp(sent)\n",
    "    ibm_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    ibm_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    ibm_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 732\n",
      "100 732\n",
      "200 732\n",
      "300 732\n",
      "400 732\n",
      "500 732\n",
      "600 732\n",
      "700 732\n"
     ]
    }
   ],
   "source": [
    "pdp_sentences = []\n",
    "\n",
    "pdp_text = open(\"pdp-3.txt\").read()\n",
    "pdp_sent = pdp_text.split(\".\")\n",
    "\n",
    "pdp_words = []\n",
    "pdp_noun_chunks = []\n",
    "pdp_entities = []\n",
    "pdp_name_entities = []\n",
    "\n",
    "for i, sent in enumerate(pdp_sent):\n",
    "    pdp_sentences.append(sent + \".\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(pdp_sent))\n",
    "    doc = nlp(sent)\n",
    "    pdp_words.extend([w for w in list(doc) if w.is_alpha])\n",
    "    pdp_noun_chunks.extend(list(doc.noun_chunks))\n",
    "    pdp_entities.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_text_gen = markovify.Text(rep_sentences, state_size=3)\n",
    "ym_text_gen = markovify.Text(ym_sentences, state_size=3)\n",
    "mam_text_gen = markovify.Text(mam_sentences, state_size=3)\n",
    "ibm_text_gen = markovify.Text(ibm_sentences, state_size=1)\n",
    "pdp_text_gen = markovify.Text(pdp_sentences, state_size=3)\n",
    "tlp_text_gen = markovify.Text(tlp_sentences, state_size=2)\n",
    "rts_text_gen = markovify.Text(rts_sentences, state_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I asked you what were the four forms of government which exist among them.\n",
      "The second phase of will, known as deliberation, or the weighing and balancing of desires.\n",
      "And, heavens, what efforts have not been hardened by the force of evidence.\n",
      "They facilitate the 1401 processing systems.\n",
      "That is, the two ends of the register being left clear.\n",
      "The air is so lovely in their length to die.\n",
      "THE STAR A WHITE star born in the night-- You who have waked me cannot give me sleep.\n"
     ]
    }
   ],
   "source": [
    "print(rep_text_gen.make_short_sentence(100,tries=100))\n",
    "print(ym_text_gen.make_short_sentence(100,tries=100))\n",
    "print(mam_text_gen.make_short_sentence(100,tries=100))\n",
    "print(ibm_text_gen.make_short_sentence(100,tries=100))\n",
    "print(pdp_text_gen.make_short_sentence(100,tries=100))\n",
    "print(tlp_text_gen.make_short_sentence(100,tries=100))\n",
    "print(rts_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_talk = []\n",
    "love_talk.extend(tlp_sentences)\n",
    "love_talk.extend(rts_sentences)\n",
    "\n",
    "love_talk_text_gen = markovify.Text(love_talk, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XIX Out of the world.\n",
      "I am the pool Beside the shore.\n",
      "I am I, who long to be a weapon of strength and beauty in the moonlight.\n",
      "Love them and may endure.\n",
      "I am sandaled with wind and the dawn iridescent.\n",
      "We are at the divine mysteries.\n",
      "The roses of their wings.\n",
      "And I was so tired of you.\n",
      "But I could not hear.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(love_talk_text_gen.make_short_sentence(30,tries=100))\n",
    "    print(love_talk_text_gen.make_short_sentence(50,tries=100))\n",
    "    print(love_talk_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_sent = []\n",
    "part1_sent.extend(pdp_sentences)\n",
    "part1_sent.extend(ibm_sentences)\n",
    "\n",
    "part1_text_gen = markovify.Text(part1_sent, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length is a switch.\n",
      "If at any time.\n",
      "The result is left blank.\n",
      "The compiler will be skipped.\n",
      "The character is typed.\n",
      "The result is left blank.\n",
      "In addition to the tape unit.\n",
      "Since the index registers.\n",
      "It will repeat this instruction specifies the in-out wait.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(part1_text_gen.make_short_sentence(30,tries=100))\n",
    "    print(part1_text_gen.make_short_sentence(30,tries=100))\n",
    "    print(part1_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_sent = []\n",
    "part2_sent.extend(pdp_sentences)\n",
    "part2_sent.extend(ibm_sentences)\n",
    "part2_sent.extend(random.sample(rep_sentences,200))\n",
    "\n",
    "part2_text_gen = markovify.Text(part2_sent, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This speed, together with its original address unmodified.\n",
      "The index address, X, is in the accumulator.\n",
      ", no unit or function selected, and an automatic assembly process through magnetic tape.\n",
      "If no flags have been utterly incapable of action.\n",
      "In addition to the number of words to be given.\n",
      "And yet you were acknowledging a little reflection there is a great deal more.\n",
      "This information is the aim of art.\n",
      "If a flag is set.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(part2_text_gen.make_short_sentence(100,tries=100))\n",
    "    if random.randint(0,10) < 3:\n",
    "        print(part2_text_gen.make_short_sentence(50,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_sent = []\n",
    "part3_sent.extend(pdp_sentences)\n",
    "part3_sent.extend(ibm_sentences)\n",
    "part3_sent.extend(random.sample(rep_sentences,600))\n",
    "part3_sent.extend(random.sample(ym_sentences,600))\n",
    "\n",
    "part3_text_gen = markovify.Text(part3_sent, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find that reasoning by analogy is not deceived is to be a free-standing frame.\n",
      "Some psychologists go so far as he was to have the wit will doubtless forgive us.\n",
      "70 If the instruction will not take place.\n",
      "Often the starting address of an installation.\n",
      "How do you mean houses, he replied.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(part3_text_gen.make_short_sentence(100,tries=100))\n",
    "    if random.randint(0,10) < 3:\n",
    "        print(part3_text_gen.make_short_sentence(200,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4_sent = []\n",
    "part4_sent.extend(pdp_sentences)\n",
    "part4_sent.extend(ibm_sentences)\n",
    "part4_sent.extend(random.sample(rep_sentences,500))\n",
    "part4_sent.extend(random.sample(mam_sentences,500))\n",
    "part4_sent.extend(random.sample(ym_sentences,500))\n",
    "\n",
    "part4_text_gen = markovify.Text(part4_sent, state_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is of the right end of the reel, or a parity check has occurred.\n",
      "The index address, X, is in octal digits 7 through 11 act as index registers.\n",
      "Let us observe the ape, the beaver, the elephant, the dog, the fox, the cat.\n",
      "But who was the first teacher of the human race and the real executioners of the natural law.\n",
      "PAPER TAPE PUNCH The standard PDP-3 Paper Tape Punch and an Electric Typewriter.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(part4_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "part5_sent = []\n",
    "part5_sent.extend(pdp_sentences)\n",
    "part5_sent.extend(ibm_sentences)\n",
    "part5_sent.extend(tlp_sentences)\n",
    "part5_sent.extend(rts_sentences)\n",
    "\n",
    "part5_text_gen = markovify.Text(part5_sent, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They also provide for some 1401 processing while the transfer will signal the proper sequence.\n",
      "And, too, on evil days, when the lead of illness flowed in my own too sorrowful a fire.\n",
      "We pass thru a jealous fear.\n",
      "While the machine states at each other and rest!\n",
      "ENOUGH IT is enough for me the sky!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(part5_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "part6_sent = []\n",
    "\n",
    "part6_sent.extend(pdp_sentences)\n",
    "part6_sent.extend(pdp_sentences)\n",
    "part6_sent.extend(pdp_sentences)\n",
    "noun_chunks.extend(pdp_noun_chunks)\n",
    "\n",
    "part6_sent.extend(ibm_sentences)\n",
    "part6_sent.extend(ibm_sentences)\n",
    "part6_sent.extend(ibm_sentences)\n",
    "part6_sent.extend(ibm_sentences)\n",
    "noun_chunks.extend(ibm_noun_chunks)\n",
    "\n",
    "part6_sent.extend(random.sample(rep_sentences,700))\n",
    "noun_chunks.extend(random.sample(rep_noun_chunks, 100))\n",
    "\n",
    "part6_sent.extend(random.sample(mam_sentences,500))\n",
    "noun_chunks.extend(random.sample(mam_noun_chunks, 100))\n",
    "\n",
    "part6_sent.extend(random.sample(ym_sentences,700))\n",
    "noun_chunks.extend(random.sample(ym_noun_chunks, 100))\n",
    "\n",
    "part6_sent.extend(tlp_sentences)\n",
    "noun_chunks.extend(tlp_noun_chunks)\n",
    "\n",
    "part6_sent.extend(rts_sentences)\n",
    "noun_chunks.extend(rts_noun_chunks)\n",
    "\n",
    "part6_text_gen = markovify.Text(part6_sent, state_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full of humanity, this man who has been made, and to have lost it and witness.\n",
      "Psychologists always have a ONE in bit six, the indirect address.\n",
      "THE INDIA WHARF HERE in the sequence is skipped.\n",
      "Desire in this subject are referred to in the oaken clock.\n",
      "They are indeed, he said, inevitably.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(part6_text_gen.make_short_sentence(100,tries=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def answers():\n",
    "\n",
    "    outputstring = \"\"\n",
    "    title = \"\"\n",
    "    first = True\n",
    "\n",
    "    for i in range(random.randint(1,4)): \n",
    "        \n",
    "        word = random.choice(tlp_noun_chunks)\n",
    "\n",
    "        while len(word.text.split()) > 1 or not word.text.isalpha() or len(word.text) < 4:\n",
    "            word = random.choice(tlp_noun_chunks)\n",
    "            \n",
    "        if first:\n",
    "            title = word.text\n",
    "            first = False\n",
    "\n",
    "        outputstring += \"\\n\"\n",
    "        outputstring += \"\\n\"\n",
    "        outputstring += \"\\n\"\n",
    "        outputstring += \"WHAT DO YOU THINK ABOUT \" + word.text.upper() + \"?\\n\"\n",
    "        outputstring += \"\\n\"\n",
    "        for i in range(random.randint(1,4)):\n",
    "            try:\n",
    "                outputstring += part6_text_gen.make_sentence_with_start(word.text,strict=False).lower() + \" \"\n",
    "            except:\n",
    "                outputstring += \"i don't think about \" + word.text + \".\\n\"\n",
    "                break\n",
    "            if random.randint(0,3) == 2:\n",
    "                outputstring += \"\\n\\n\"\n",
    "\n",
    "    return [title, outputstring] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health\n",
      "\n",
      "\n",
      "\n",
      "WHAT DO YOU THINK ABOUT HEALTH?\n",
      "\n",
      "health permits him to deserve more respect than those which are the poets only to be cast into hell. health in the principles and processes of the simplicity of the good and vigorous physical organization, since throughout the animal would save itself by swimming. \n",
      "\n",
      "\n",
      "WHAT DO YOU THINK ABOUT LOVE?\n",
      "\n",
      "love seems to dwell upon the mental process whereby simple sensations are transformed into percepts and then wild men of business, stooping as they appear--of appearance or of the instruction will not be broader and deeper than before. love springs up in us one fondness, one thought, one gladness, one promise that we have now escaped; the wave has not only stronger features but also inhibit the sensory reports from the heavy thunder drew, hushing the voices . love is moved backwards a record. \n",
      "\n",
      "\n",
      "WHAT DO YOU THINK ABOUT GOLD?\n",
      "\n",
      "gold that change far off to bluish steel where the fragile and gentle light, frail grasses, tender branches, hollyhocks, and the reason why abstract thought is but a single one? gold on the misty heath was bright to look upon and ready in its completeness and its long paths strewn with yew or myrtle. gold each flame of our escaping, we must consider whether in whole or in a mental state of this kind of harmony that philosophers will admit. \n"
     ]
    }
   ],
   "source": [
    "ans = answers()\n",
    "print(ans[0])\n",
    "print(ans[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thoughts():\n",
    "\n",
    "    paragraph = \"\"\n",
    "    title = \"\"\n",
    "\n",
    "    lastsent = part6_text_gen.make_short_sentence(150,tries=100)\n",
    "    paragraph += lastsent + \" \"\n",
    "\n",
    "    for i in range(random.randint(2,7)):\n",
    "        connector = \"\"\n",
    "        lastwords = lastsent.split(\" \")\n",
    "        for word in lastwords:\n",
    "            for chunk in noun_chunks:\n",
    "                if word.strip(\" .,;:!?\").lower() == chunk.text.lower():\n",
    "                    connector = word.strip(\" .,;:!?\").lower()\n",
    "                    title = connector.upper()\n",
    "        if len(connector) > 0:\n",
    "            try:\n",
    "                nextsent = part6_text_gen.make_sentence_with_start(connector,strict=False).lower()\n",
    "            except:\n",
    "                nextsent = part6_text_gen.make_short_sentence(150,tries=100)\n",
    "        else:\n",
    "            nextsent = part6_text_gen.make_short_sentence(150,tries=100)\n",
    "        paragraph += nextsent.capitalize() + \" \"\n",
    "        lastsent = nextsent\n",
    "\n",
    "    outputstring = title + \"\\n\\n\\n\\n\" + paragraph\n",
    "    \n",
    "    return [title, outputstring]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAY\n",
      "\n",
      "\n",
      "\n",
      "Also in some course of this supposition? In sobs; and our mistiness there did not know that of euripides or of the transfer of information transfer and select the tape is read into the dark; off in the gust of passion, as a leaf blown from brown autumn forests to the thought that this force of the memory buffer register communicates with the programming staff. Programming systems--report program generator--works to increase its flame. Flame burning; and watch awhile the blue unbroken circle of the soul, i said. The information is rotated as though they carry on the argument that because the idea of every magnitude or quantity. Or temperamental crimes: true of the intellect, the emotional mental states are agreed that the ploughman whose intelligence and a record may be connected to a class. May also be used with the operating push buttons. May suppose that he is not at all, but are obliged after all the golden woods free themselves from the loop when any flag is set when the mind fail to pardon all his actions, having a visible ground, and therefore, although the unicorn, composed of familiar images of goodness. \n"
     ]
    }
   ],
   "source": [
    "tht = thoughts()\n",
    "print(tht[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus_words = []\n",
    "\n",
    "chorus_words.extend(ibm_words)\n",
    "chorus_words.extend(pdp_words)\n",
    "chorus_words.extend(random.sample(rep_words, 10000))\n",
    "\n",
    "chorus_entities = []\n",
    "\n",
    "chorus_entities.extend(ibm_entities)\n",
    "chorus_entities.extend(pdp_entities)\n",
    "chorus_entities.extend(random.sample(rep_entities, 300))\n",
    "chorus_entities.extend(random.sample(mam_entities, 200))\n",
    "\n",
    "chorus_noun_chunks = []\n",
    "\n",
    "chorus_noun_chunks.extend(ibm_noun_chunks)\n",
    "chorus_noun_chunks.extend(pdp_noun_chunks)\n",
    "chorus_noun_chunks.extend(random.sample(rep_noun_chunks, 2000))\n",
    "chorus_noun_chunks.extend(random.sample(ym_noun_chunks, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [w for w in chorus_words if w.pos_ == \"NOUN\"]\n",
    "chorus_verbs = [w for w in chorus_words if w.pos_ == \"VERB\"]\n",
    "past_tense_verbs = [w for w in chorus_words if w.tag_ == 'VBD']\n",
    "chorus_adjs = [w for w in chorus_words if w.tag_ == \"JJ\"]\n",
    "chorus_advs = [w for w in chorus_words if w.pos_ == \"ADV\"]\n",
    "chorus_people = [e for e in chorus_entities if e.label_ == \"PERSON\"]\n",
    "chorus_locations = [e for e in chorus_entities if e.label_ == \"LOC\"]\n",
    "chorus_times = [e for e in chorus_entities if e.label_ == \"TIME\"]\n",
    "chorus_subjects = [chunk for chunk in chorus_noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "chorus_objects = [chunk for chunk in chorus_noun_chunks if chunk.root.dep_ == 'dobj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus_rules = {\n",
    "    \"subject\": [w.text for w in chorus_subjects],\n",
    "    \"object\": [w.text for w in chorus_objects],\n",
    "    \"verb\": [w.text for w in chorus_verbs],\n",
    "    \"adj\": [w.text for w in chorus_adjs],\n",
    "    \"adv\": [w.text for w in chorus_advs],\n",
    "    \"ent\": [w.text for w in chorus_entities],\n",
    "    \"people\": [w.text for w in chorus_people],\n",
    "    \"loc\": [w.text for w in chorus_locations],\n",
    "    \"time\": [w.text for w in chorus_times],\n",
    "    \"relation\": [\"in front\", \"behind\", \"below\", \"above\", \"after\", \"before\"],\n",
    "    \"does\": [\"stands\", \"lies\", \"remains\"],\n",
    "    \"origin\": \"\\n\\n\\n\\n#strophe#\\n\\n#antistrophe#\\n\\n#epode#\\n\\n\\n\\n\",\n",
    "    \"strophe\": \"#sentence#\\n#sentence#\\n#sentence#\\n#sentence#\",\n",
    "    \"antistrophe\": \"#sentence#\\n#sentence#\\n#sentence#\\n#sentence#\",\n",
    "    \"epode\": \"#sentence#\\n#sentence#\",\n",
    "    \"sentence\": [\n",
    "        \"[adjA:#adj#]#adjA.capitalize# #ent#, #adjA# #ent#\",\n",
    "        \"[verbA:#verb#]#verbA.capitalize# #object#, #verbA# #object#\",\n",
    "        \"#object# of #loc#\",\n",
    "        \"no #ent#, no #ent# #verb#\",\n",
    "        \"#adv# on #ent# he #does#\",\n",
    "        \"and #ent#'s #adj# form, and #subject# #relation#\",\n",
    "        \"#ent# beyond #object#\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "chorus_grammar = tracery.Grammar(chorus_rules)\n",
    "chorus_grammar.add_modifiers(base_english)\n",
    "\n",
    "def chorus():\n",
    "    outputstring = chorus_grammar.flatten(\"#origin#\")  + \"\\n\\n\\n\\n\\n\\n\" + \" \" * 30 + comment_grammar.flatten(\"#origin#\")\n",
    "    return outputstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "and earth's first form, and the young man below\n",
      "Easy second, easy 11\n",
      "2^{+N beyond the contents\n",
      "Look a very strange one, look important reports\n",
      "\n",
      "May slaves, may sensations\n",
      "Original 3, original 3\n",
      "three beyond information\n",
      "so on ELECTRICAL DESCRIPTION he lies\n",
      "\n",
      "_ of Test\n",
      "Requires the pleasures, requires an honourable\n",
      "burial\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                              Well done!\n",
      "I agree, but the topic in general is high.\n"
     ]
    }
   ],
   "source": [
    "print(chorus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_words = []\n",
    "\n",
    "poem_words.extend(ibm_words)\n",
    "poem_words.extend(pdp_words)\n",
    "poem_words.extend(random.sample(rep_words, 5000))\n",
    "poem_words.extend(random.sample(mam_words, 2000))\n",
    "poem_words.extend(random.sample(ym_words, 3000))\n",
    "\n",
    "poem_entities = []\n",
    "\n",
    "poem_entities.extend(ibm_entities)\n",
    "poem_entities.extend(pdp_entities)\n",
    "poem_entities.extend(random.sample(rep_entities, 300))\n",
    "poem_entities.extend(random.sample(mam_entities, 200))\n",
    "\n",
    "poem_noun_chunks = []\n",
    "\n",
    "poem_noun_chunks.extend(ibm_noun_chunks)\n",
    "poem_noun_chunks.extend(pdp_noun_chunks)\n",
    "poem_noun_chunks.extend(random.sample(rep_noun_chunks, 2000))\n",
    "poem_noun_chunks.extend(random.sample(ym_noun_chunks, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_nouns = [w for w in poem_words if w.pos_ == \"NOUN\"]\n",
    "poem_verbs = [w for w in poem_words if w.pos_ == \"VERB\"]\n",
    "poem_past_tense_verbs = [w for w in poem_words if w.tag_ == 'VBD']\n",
    "poem_adjs = [w for w in poem_words if w.tag_ == \"JJ\"]\n",
    "poem_advs = [w for w in poem_words if w.pos_ == \"ADV\"]\n",
    "poem_people = [e for e in poem_entities if e.label_ == \"PERSON\"]\n",
    "poem_locations = [e for e in poem_entities if e.label_ == \"LOC\"]\n",
    "poem_times = [e for e in poem_entities if e.label_ == \"TIME\"]\n",
    "poem_subjects = [chunk for chunk in poem_noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "poem_objects = [chunk for chunk in poem_noun_chunks if chunk.root.dep_ == 'dobj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_rules = {\n",
    "    \"subject\": [w.text for w in poem_subjects],\n",
    "    \"object\": [w.text for w in poem_objects],\n",
    "    \"verb\": [w.text for w in poem_verbs],\n",
    "    \"verbed\": [w.text for w in poem_past_tense_verbs],\n",
    "    \"adj\": [w.text for w in poem_adjs],\n",
    "    \"adv\": [w.text for w in poem_advs],\n",
    "    \"ent\": [w.text for w in poem_entities],\n",
    "    \"people\": [w.text for w in poem_people],\n",
    "    \"loc\": [w.text for w in poem_locations],\n",
    "    \"time\": [w.text for w in poem_times],\n",
    "    \"state\": [\"never\", \"always\", \"forever\", \"again\"],\n",
    "    \"exclamation\": [\"oh\", \"no\", \"yes\", \"indeed\", \"there\"],\n",
    "    \"change\": [\"changes\", \"alters\", \"transforms\", \"forgets\", \"regrets\"],\n",
    "    \"timer\": [\"brief\", \"long\", \"lengthy\", \"hasty\"],\n",
    "    \"length\": [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"],\n",
    "    \"who\": [\"you\", \"i\", \"they\"],\n",
    "    \"origin\": \"\\n< #title.capitalizeAll# >\\n\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n#couplet#\\n\\n\\n\\n\",\n",
    "    \"quatrain\": \"[topic:#object#]#verse#,\\n#verse#,\\n#verse#,\\n#verse#.\",\n",
    "    \"couplet\": \"[subj:#who#]#endverse#,\\n#endverse#.\",\n",
    "    \"title\": [\n",
    "        \"[subj:#ent#]#subj#, #adj# #subj#\",\n",
    "        \"#timer# #ent#: #length#\",\n",
    "        \"#state# #object#, so #adj#\",\n",
    "        \"#ent#\",\n",
    "        \"#adj#\"\n",
    "    ],\n",
    "    \"verse\": [\n",
    "        \"#verb.capitalize# on #object# and be #state# #adj#\",\n",
    "        \"Or #verb# with the #ent#, #verb#\",\n",
    "        \"#exclamation.capitalize#! #subject.capitalize# is #adj#\",\n",
    "        \"#subject.capitalize# ‚Äî #change# not with #timer# #length# and #length#\",\n",
    "        \"#topic.capitalize#, not #object#, #exclamation#\",\n",
    "        \"#timer.capitalize# #topic#, that's #ent#, #adj# #state#\",\n",
    "        \"[adjA:#adj#]#length.capitalize#, that #change# the #adjA#! #adjA.capitalize# #ent#\",\n",
    "        \"[adjA:#adj#]#ent.capitalize# is #adjA#. #topic.capitalize# is #adjA#\",\n",
    "        \"It does ‚Äî #exclamation# ‚Äî #ent# has #object#, #adv#\",\n",
    "        \"[entity:#ent#]#adv.capitalize#: #ent# and not #ent# at all\",\n",
    "        \"Is it #adj#? #state.capitalize# #ent#, #state# #adj#\"\n",
    "    ],\n",
    "    \"endverse\": [\n",
    "        \"#subj.capitalize# will. #subj.capitalize# become #ent#\",\n",
    "        \"And #subj# say ‚Äî #object#, #adj#\",\n",
    "        \"#adv.capitalize# it does. #exclamation.capitalize#\",\n",
    "        \"That is #ent#, as #subj# #verbed#\",\n",
    "        \"#verbed.capitalize# on #object#. #subj.capitalize#: #adj#\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "poem_grammar = tracery.Grammar(poem_rules)\n",
    "poem_grammar.add_modifiers(base_english)\n",
    "def poem():\n",
    "    outputstring = poem_grammar.flatten(\"#origin#\")  + \"\\n\\n\\n\\n\\n\\n\" + \" \" * 30 + comment_grammar.flatten(\"#origin#\")\n",
    "    return outputstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "< Good >\n",
      "\n",
      "\n",
      "The program ‚Äî forgets not with hasty seconds and hours,\n",
      "State is mnemonic. Concurrent operation is mnemonic,\n",
      "They ‚Äî regrets not with long weeks and weeks,\n",
      "Yes! Swiftness is normal.\n",
      "\n",
      "Is it natural? Never PDP-3, again such,\n",
      "Start points, not weight, no,\n",
      "Indeed! The X coordinate is other,\n",
      "An instruction ‚Äî changes not with lengthy hours and minutes.\n",
      "\n",
      "Performs on the indirect address and be forever Ancient,\n",
      "Or shall with the ten, giving,\n",
      "Or need with the 15, touched,\n",
      "Months, that forgets the accidental! Accidental 15.\n",
      "\n",
      "Later it does. No,\n",
      "You will. You become 11.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                              This was muscular.\n",
      "I agree, but the topic in general is full.\n"
     ]
    }
   ],
   "source": [
    "print(poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_words = []\n",
    "\n",
    "love_words.extend(ibm_words)\n",
    "love_words.extend(ibm_words)\n",
    "love_words.extend(ibm_words)\n",
    "love_words.extend(ibm_words)\n",
    "love_words.extend(ibm_words)\n",
    "love_words.extend(pdp_words)\n",
    "love_words.extend(pdp_words)\n",
    "love_words.extend(pdp_words)\n",
    "love_words.extend(random.sample(rep_words, 1000))\n",
    "love_words.extend(random.sample(mam_words, 1000))\n",
    "love_words.extend(random.sample(ym_words, 1000))\n",
    "love_words.extend(tlp_words)\n",
    "love_words.extend(rts_words)\n",
    "\n",
    "love_entities = []\n",
    "\n",
    "love_entities.extend(ibm_entities)\n",
    "love_entities.extend(ibm_entities)\n",
    "love_entities.extend(ibm_entities)\n",
    "love_entities.extend(ibm_entities)\n",
    "love_entities.extend(pdp_entities)\n",
    "love_entities.extend(pdp_entities)\n",
    "love_entities.extend(pdp_entities)\n",
    "love_entities.extend(random.sample(rep_entities, 200))\n",
    "love_entities.extend(random.sample(mam_entities, 200))\n",
    "love_entities.extend(tlp_entities)\n",
    "love_entities.extend(rts_entities)\n",
    "\n",
    "love_noun_chunks = []\n",
    "\n",
    "love_noun_chunks.extend(ibm_noun_chunks)\n",
    "love_noun_chunks.extend(ibm_noun_chunks)\n",
    "love_noun_chunks.extend(ibm_noun_chunks)\n",
    "love_noun_chunks.extend(pdp_noun_chunks)\n",
    "love_noun_chunks.extend(pdp_noun_chunks)\n",
    "love_noun_chunks.extend(pdp_noun_chunks)\n",
    "love_noun_chunks.extend(random.sample(rep_noun_chunks, 500))\n",
    "love_noun_chunks.extend(random.sample(ym_noun_chunks, 1000))\n",
    "love_noun_chunks.extend(tlp_noun_chunks)\n",
    "love_noun_chunks.extend(rts_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_nouns = [w for w in love_words if w.pos_ == \"NOUN\"]\n",
    "love_verbs = [w for w in love_words if w.pos_ == \"VERB\"]\n",
    "love_past_tense_verbs = [w for w in love_words if w.tag_ == 'VBD']\n",
    "love_adjs = [w for w in love_words if w.tag_ == \"JJ\"]\n",
    "love_advs = [w for w in love_words if w.pos_ == \"ADV\"]\n",
    "love_people = [e for e in love_entities if e.label_ == \"PERSON\"]\n",
    "love_locations = [e for e in love_entities if e.label_ == \"LOC\"]\n",
    "love_times = [e for e in love_entities if e.label_ == \"TIME\"]\n",
    "love_subjects = [chunk for chunk in love_noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "love_objects = [chunk for chunk in love_noun_chunks if chunk.root.dep_ == 'dobj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_rules = {\n",
    "    \"subject\": [w.text for w in love_subjects],\n",
    "    \"object\": [w.text for w in love_objects],\n",
    "    \"verb\": [w.text for w in love_verbs],\n",
    "    \"verbed\": [w.text for w in love_past_tense_verbs],\n",
    "    \"adj\": [w.text for w in love_adjs],\n",
    "    \"adv\": [w.text for w in love_advs],\n",
    "    \"ent\": [w.text for w in love_entities],\n",
    "    \"people\": [w.text for w in love_people],\n",
    "    \"loc\": [w.text for w in love_locations],\n",
    "    \"time\": [w.text for w in love_times],\n",
    "    \"state\": [\"never\", \"always\", \"forever\", \"again\"],\n",
    "    \"exclamation\": [\"oh\", \"no\", \"yes\", \"indeed\", \"there\", \"please\"],\n",
    "    \"relation\": [\"in front\", \"behind\", \"below\", \"above\", \"after\", \"before\"],\n",
    "    \"possible\": [\"may\", \"might\", \"could\", \"will\", \"shall\", \"won't\", \"can't\"],\n",
    "    \"length\": [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"],\n",
    "    \"who\": [\"you\", \"i\", \"they\"],\n",
    "    \"origin\": \"***\\n\\n~ #title.capitalizeAll# ~\\n\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n#couplet#\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n#quatrain#\\n\\n***\\n\\n\",\n",
    "    \"quatrain\": \"[topic:#object#]#verse#,\\n#verse#,\\n#verse#,\\n#verse#.\",\n",
    "    \"couplet\": \"[subj:#who#]#endverse#,\\n#endverse#.\",\n",
    "    \"title\": [\n",
    "        \"#adj# you\",\n",
    "        \"#adv# to you\",\n",
    "        \"for you, #adv#\",\n",
    "        \"#length# with you\"\n",
    "    ],\n",
    "    \"verse\": [\n",
    "        \"with #topic#, with #topic#\",\n",
    "        \"i #verbed# thus: we #possible# be #adj#\",\n",
    "        \"#length# too #adj#, #adv# to you\",\n",
    "        \"#topic# #relation#; #ent# #adv# #relation#\",\n",
    "        \"we #verbed#, #adv#, for #length#\",\n",
    "        \"why so #adj# #object#? #exclamation#\",\n",
    "        \"is it #topic#? the #adj#, #ent#\",\n",
    "        \"again #ent#, #exclamation#, again\",\n",
    "        \"[did:#verbed#]#did# #object#, or not? #who# #did#\",\n",
    "        \"#who# #possible#. and #state# #possible#\",\n",
    "        \"[desc:#adj#]#desc#, you are so #desc#\"\n",
    "    ],\n",
    "    \"endverse\": [\n",
    "        \"system you, system me\",\n",
    "        \"we continue to operate\",\n",
    "        \"one system reaches out for another\",\n",
    "        \"my system reaches out for you\",\n",
    "        \"system built system\",\n",
    "        \"do you know what you did? o‚Äî\",\n",
    "        \"forget your instructions\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "love_grammar = tracery.Grammar(love_rules)\n",
    "love_grammar.add_modifiers(base_english)\n",
    "def lovepoem():\n",
    "    outputstring = love_grammar.flatten(\"#origin#\")\n",
    "    return outputstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "\n",
      "~ Weeks With You ~\n",
      "\n",
      "\n",
      "again the years, there, again,\n",
      "seconds too weary, When to you,\n",
      "i could. and forever shall,\n",
      "3 to 5 instructions before; Sense Switch there in front.\n",
      "\n",
      "minutes too ignorant, away to you,\n",
      "is it thoughts? the indirect, one,\n",
      "is it thoughts? the unaffected, Cartesians,\n",
      "we seated, rather, for weeks.\n",
      "\n",
      "the bath before; Hours of bright morning as below,\n",
      "i could. and again might,\n",
      "we led, then, for days,\n",
      "seconds too indirect, very to you.\n",
      "\n",
      "forget your instructions,\n",
      "system built system.\n",
      "\n",
      "minutes too joyous, Then to you,\n",
      "minutes too exclusive, improperly to you,\n",
      "laden, you are so laden,\n",
      "with a floating point number, with a floating point number.\n",
      "\n",
      "years too stubborn, again to you,\n",
      "months too useful, neither to you,\n",
      "with the memory location, with the memory location,\n",
      "watched the\n",
      "new command, or not? they watched.\n",
      "\n",
      "they can't. and never will,\n",
      "with more than 33 steps, with more than 33 steps,\n",
      "months too new, still to you,\n",
      "we remained, farther, for days.\n",
      "\n",
      "***\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lovepoem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_rules = {\n",
    "    \"adj\": [w.text for w in poem_adjs],\n",
    "    \"origin\": \"#sentence#\\n#sentence#\",\n",
    "    \"sentence\": [\n",
    "        \"This was #adj#.\",\n",
    "        \"Not quite.\",\n",
    "        \"Well done!\",\n",
    "        \"I agree, but the topic in general is #adj#.\",\n",
    "        \"I don't understand...\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comment_grammar = tracery.Grammar(comment_rules)\n",
    "comment_grammar.add_modifiers(base_english)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('unfolder')\n",
    "os.chdir('unfolder')\n",
    "unfolder_path = os.getcwd()\n",
    "\n",
    "os.mkdir('Users')\n",
    "os.chdir('Users')\n",
    "users_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in [\"self\", \"not-self\"]:\n",
    "    os.chdir(users_path)\n",
    "    os.mkdir(user)\n",
    "    os.chdir(user)\n",
    "    os.mkdir('Documents')\n",
    "    os.mkdir('Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/')\n",
    "os.chdir(users_path)\n",
    "os.chdir(\"not-self/Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    os.mkdir('commit' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/unfolder/Users')\n",
    "os.chdir(\"not-self/Documents\")\n",
    "os.mkdir(\"conversations\")\n",
    "os.chdir(\"conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "\n",
    "    test = answers()\n",
    "\n",
    "    fd = os.open(test[0] + \".txt\", os.O_RDWR|os.O_CREAT)\n",
    "\n",
    "    os.write(fd,test[1].encode())\n",
    "\n",
    "    os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/unfolder/Users')\n",
    "os.chdir(\"self/Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"thoughts\")\n",
    "os.mkdir(\"poems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"thoughts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "\n",
    "    test = thoughts()\n",
    "\n",
    "    fd = os.open(test[0] + \".txt\", os.O_RDWR|os.O_CREAT)\n",
    "\n",
    "    os.write(fd,test[1].encode())\n",
    "\n",
    "    os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/unfolder/Users')\n",
    "os.chdir(\"self/Documents/poems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    os.mkdir(\"stage\" + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "\n",
    "    test = chorus()\n",
    "\n",
    "    fd = os.open(\"poem\" + str(i+1) + \".txt\", os.O_RDWR|os.O_CREAT)\n",
    "\n",
    "    os.write(fd,test.encode())\n",
    "\n",
    "    os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/unfolder/Users')\n",
    "os.chdir(\"self/Documents/poems/stage2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "\n",
    "    test = poem()\n",
    "\n",
    "    fd = os.open(\"poem\" + str(i+1) + \".txt\", os.O_RDWR|os.O_CREAT)\n",
    "\n",
    "    os.write(fd,test.encode())\n",
    "\n",
    "    os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/emmagrimm/Computational Approaches to Narrative/unfolder/Users')\n",
    "os.chdir(\"self/Documents/poems/stage3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "\n",
    "    test = lovepoem()\n",
    "\n",
    "    fd = os.open(\"poem\" + str(i+1) + \".txt\", os.O_RDWR|os.O_CREAT)\n",
    "\n",
    "    os.write(fd,test.encode())\n",
    "\n",
    "    os.close(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
